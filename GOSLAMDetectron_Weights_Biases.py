# -*- coding: utf-8 -*-
"""_GOSLAMDetectron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qQ4hJ50xpzKtzG4yid7qWQeiM97samuY
"""

# Install dependencies
!pip install pyyaml==5.1
!pip install opencv-python-headless

# Install other necessary libraries
!pip install tensorboard

# install weights and biasses
!pip install wandb -qU

import sys, os, distutils.core

# download detr2 model
!git clone 'https://github.com/facebookresearch/detectron2'

# install dependencies
dist = distutils.core.run_setup("./detectron2/setup.py")
!python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}

sys.path.insert(0, os.path.abspath('./detectron2'))

import wandb

# this needs to be in a separate cell ¯\_(ツ)_/¯
# The key is specific to the project and should not be used anywhere else
wandb.login(key="f9ef21714bde357dd61c15df3f3852899ecf0ab2")

# ensure a GPU is detectable and working
import torch, detectron2
!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Commented out IPython magic to ensure Python compatibility.
# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

FOLDERNAME = 'ColabNotebooks/cs231nfinalproject/datasets/'
assert FOLDERNAME is not None, "[!] Enter the foldername."

import sys
sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))

# %cd /content/drive/My\ Drive/$FOLDERNAME
# %ls /content/drive/My\ Drive/$FOLDERNAME

"""## Load custom dataset..."""

# !cp /content/drive/My\ Drive/$FOLDERNAME/merged_dataset.zip /content/dataset.zip

# # Unzip the dataset
# !unzip -qq /content/dataset.zip -d /content/dataset
!ls /content/dataset

"""## Initialization and register datasets"""

from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.data.datasets import register_coco_instances
from pathlib import Path

# get the merged dataset and make sure the path variable is correct
dataset_path = Path("/content/dataset/merged_dataset")
annotations_path = dataset_path / "annotations"

assert dataset_path.exists(), f"Dataset path does not exist. \n{dataset_path=}"
assert annotations_path.exists(), f"Annotations path does not exist. \n{annotations_path=}"

training_dataset = "custom_train"
validation_dataset = "custom_val"
test_dataset = "custom_test"
all_datasets = [training_dataset, validation_dataset, test_dataset]


# Register the custom datasets in detr
def unregister_dataset(name):
    if name in MetadataCatalog:
        del MetadataCatalog[name]
    if name in DatasetCatalog:
        del DatasetCatalog[name]

def register_dataset(name, json_file:Path, image_root:Path):
    assert json_file.is_file(), f"JSON file does not exist at:{json_file}"
    assert image_root.is_dir(), f"Image root does not exist at: {image_root}"
    unregister_dataset(name)  # Unregister the dataset if already registered
    print(f"\nRegistering dataset: {name} ...")
    register_coco_instances(name, {}, str(json_file), str(image_root))
    print(f"Registered {name} dataset!")


for dataset in all_datasets:
  ret = register_dataset(
      name=dataset,
      json_file=annotations_path / f"{dataset}.json",
      image_root=dataset_path / dataset.replace("custom_", ""),
  )

print(f"-----------------\n# Registered datasets: {len(MetadataCatalog)}")

"""## Train"""

import random
import matplotlib.pyplot as plt
import math
import cv2
from detectron2.utils.visualizer import Visualizer

# initialize wandb
wandb.init(
    project="detr2-indoors",
    job_type=f"detr_training",
    config={}
)

training_dataset_dicts = DatasetCatalog.get(training_dataset)
training_metadata = MetadataCatalog.get(training_dataset)

# Log dataset information
wandb.config.update({
    "dataset_name": training_dataset,
    "num_samples": len(training_dataset_dicts),
    "labels": training_metadata.thing_classes
})

num_sample_images = 5

sample_images = []
# Log example images from the training data
for i in range(5):  # Log 5 examples
    d = random.choice(training_dataset_dicts)
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=training_metadata, scale=0.5)
    out = visualizer.draw_dataset_dict(d)
    img_with_annotations = out.get_image()[:, :, ::-1]

    # Log the image with annotations to wandb
    wandb.log({"examples": [wandb.Image(img_with_annotations, caption=d["file_name"])]})
    sample_images.append(img_with_annotations)

grid_dim = math.floor(math.sqrt(num_sample_images))
# visualize the sample images
fig, axes = plt.subplots(grid_dim, grid_dim, figsize=(15, 15))
axes = axes.flatten()

for img, ax in zip(sample_images, axes):
    ax.imshow(img)
    ax.axis('off')  # Hide the axis

# Remove empty subplots if there are any
for ax in axes[len(sample_images):]:
    fig.delaxes(ax)

plt.tight_layout()
plt.show()

from detectron2.config import get_cfg
from detectron2 import model_zoo

# model configs
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = (training_dataset,)
cfg.DATASETS.TEST = (validation_dataset,)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
cfg.SOLVER.IMS_PER_BATCH = 10
cfg.SOLVER.BASE_LR = 0.01
cfg.SOLVER.MAX_ITER = 1000
cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(training_metadata.thing_classes)
cfg.OUTPUT_DIR = "./outputpal"


# log the config
wandb.config.update(cfg)

import torch
import numpy as np
from detectron2.engine import DefaultTrainer, HookBase
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader
from detectron2.checkpoint import DetectionCheckpointer
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from io import BytesIO

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class FeatureExtractor(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model.to(device)  # Move model to the appropriate device

    def forward(self, images):
        images = images.to(device)  # Move images to the same device as the model
        with torch.no_grad():
            features = self.model.backbone(images)
        return features

# Adjust collect_features based on actual batch structure
def collect_features(data_loader, feature_extractor):
    features_list = []
    labels_list = []

    for batch in data_loader:
        for data in batch:
            images = data["image"].to(device).float() / 255.0  # Convert to float and normalize
            instances = data["instances"]
            labels = instances.gt_classes.to(device)  # Ensure the labels are on the same device
            features = feature_extractor(images.unsqueeze(0))  # Unsqueeze to add batch dimension

            features_list.append(features["0"].cpu().numpy())
            labels_list.append(labels.cpu().numpy())  # Move to CPU for numpy operations

    features_np = np.concatenate(features_list, axis=0)
    labels_np = np.concatenate(labels_list, axis=0)

    return features_np, labels_np

def apply_tsne(features_np):
    tsne = TSNE(n_components=2, random_state=42)
    features_tsne = tsne.fit_transform(features_np)
    return features_tsne

def log_tsne_plot(features_tsne, labels_np):
    df = pd.DataFrame(features_tsne, columns=["x", "y"])
    df["label"] = labels_np

    plt.figure(figsize=(10, 10))
    sns.scatterplot(x="x", y="y", hue="label", palette="tab10", data=df, legend="full", alpha=0.6)
    plt.title("t-SNE of Training Dataset Features")
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")

    buf = BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)

    wandb.log({"t-SNE plot": wandb.Image(buf)})

    plt.clf()

class TrainerWithTsne(DefaultTrainer):
    def __init__(self, cfg):
        super().__init__(cfg)
        self.feature_extractor = FeatureExtractor(self.model)

    # def after_step(self):
    #     super().after_step()
    #     if self.iter % 100 == 0:
    #         data_loader = build_detection_train_loader(self.cfg)
    #         features_np, labels_np = collect_features(data_loader, self.feature_extractor)
    #         features_tsne = apply_tsne(features_np)
    #         log_tsne_plot(features_tsne, labels_np)

    def build_hooks(self):
        hooks = super().build_hooks()
        hooks.insert(-1, WandbHook())
        return hooks

# We care about the following losses in the detr log
# - total_loss is the overall loss being minimized during training.
# - loss_cls is the classification loss for the final object detection stage.
# - loss_box_reg is the bounding box regression loss for the final object detection stage.
# - loss_rpn_cls is the classification loss for the Region Proposal Network .
# - loss_rpn_loc is the localization loss for the Region Proposal Network .
readable_metric_map = {
    "total_loss": "Total loss",
    "loss_cls": "Classification loss",
    "loss_box_reg": "BoundingBox regression loss",
    "loss_rpn_cls": "RPN classification loss",
    "loss_rpn_loc": "RPN localization loss",
    "lr": "learning rate",
}

class WandbHook(HookBase):
    def after_step(self):
        # Log metrics to wandb
        metrics = self.trainer.storage.latest()
        wandb_training_metrics = {}

        for metric in metrics:
            # don't need to track time..
            if "time" in metric:
                continue
            # replace the metric keys with more readable keys
            if metric in readable_metric_map:
                wandb_training_metrics[readable_metric_map[metric]] = metrics[metric][0]
            else:
                wandb_training_metrics[metric] = metrics[metric][0]

        wandb.log(wandb_training_metrics)

# train
trainer = TrainerWithTsne(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

wandb.finish()

"""## Test"""

from detectron2.engine import DefaultPredictor

# initialize wandb
wandb.init(
    project="detr2-indoors",
    job_type=f"detr_testing",
    config={}
)

metadata = MetadataCatalog.get(test_dataset)
dataset_dicts = DatasetCatalog.get(test_dataset)

cfg = get_cfg()
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg.DATASETS.TEST = (test_dataset,)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(metadata.thing_classes)

# log the config
wandb.config.update(cfg)

predictor = DefaultPredictor(cfg)

# Function to run inference and log results to wandb
def run_inference_and_log(dataset_dicts, metadata, predictor):
    for d in dataset_dicts:
        img = cv2.imread(d["file_name"])
        outputs = predictor(img)

        # Visualize the predictions
        v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)
        out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

        # Log the image with predictions to wandb
        wandb.log({"inference": [wandb.Image(out.get_image()[:, :, ::-1], caption=d["file_name"])]})

# Run inference and log results
run_inference_and_log(dataset_dicts, metadata, predictor)

# Finish the wandb run
wandb.finish()

import wandb
import torch
import numpy as np
from detectron2.engine import DefaultPredictor, DefaultTrainer
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader
from detectron2.checkpoint import DetectionCheckpointer
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from io import BytesIO

# Initialize wandb
wandb.init(
    project="detr2-indoors",
    job_type=f"detr_tSNE",
    config={}
)

metadata = MetadataCatalog.get(training_dataset)
dataset_dicts = DatasetCatalog.get(training_dataset)

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a custom testing threshold
cfg.DATASETS.TEST = (test_dataset,)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(metadata.thing_classes)

predictor = DefaultPredictor(cfg)

# Identify the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class FeatureExtractor(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model.to(device)  # Move model to the appropriate device

    def forward(self, images):
        images = images.to(device).float() / 255.0  # Convert to float and normalize
        with torch.no_grad():
            features = self.model.backbone(images)
        return features

# Initialize the feature extractor with the predictor model
feature_extractor = FeatureExtractor(predictor.model)

def extract_features(data_loader, feature_extractor):
    features_list = []
    labels_list = []

    for batch in data_loader:
        for data in batch:
            images = data["image"].to(device).float() / 255.0  # Convert to float and normalize
            instances = data["instances"]
            labels = instances.gt_classes.to(device)  # Ensure the labels are on the same device
            features = feature_extractor(images.unsqueeze(0))  # Unsqueeze to add batch dimension

            features_list.append(features.cpu().numpy())  # Move to CPU for numpy operations
            labels_list.append(labels.cpu().numpy())  # Move to CPU for numpy operations

    features_np = np.concatenate(features_list, axis=0)
    labels_np = np.concatenate(labels_list, axis=0)

    return features_np, labels_np

def apply_tsne(features_np):
    tsne = TSNE(n_components=2, random_state=42)
    features_tsne = tsne.fit_transform(features_np)
    return features_tsne

def log_tsne_plot(features_tsne, labels_np):
    df = pd.DataFrame(features_tsne, columns=["x", "y"])
    df["label"] = labels_np

    plt.figure(figsize=(10, 10))
    sns.scatterplot(x="x", y="y", hue="label", palette="tab10", data=df, legend="full", alpha=0.6)
    plt.title("t-SNE of Training Dataset Features")
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")

    buf = BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)

    wandb.log({"t-SNE plot": wandb.Image(buf)})

    plt.clf()

# Build data loader
data_loader = build_detection_train_loader(cfg)

# Extract features and apply t-SNE
features_np, labels_np = extract_features(data_loader, feature_extractor)
features_tsne = apply_tsne(features_np)

# Log t-SNE plot to wandb
log_tsne_plot(features_tsne, labels_np)

# Finish the wandb run
wandb.finish()

from detectron2.engine import DefaultPredictor

# Evaluation setup
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model
cfg.DATASETS.TEST = (test_dataset,)
predictor = DefaultPredictor(cfg)

from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

evaluator = COCOEvaluator(test_dataset, cfg, False, output_dir="./outputpal/")
val_loader = build_detection_test_loader(cfg, test_dataset)
evaluation_results = inference_on_dataset(predictor.model, val_loader, evaluator)
print(evaluation_results)

# Plotting and saving predictions
test_metadata = MetadataCatalog.get(test_dataset)
test_dataset_dicts = DatasetCatalog.get(test_dataset)

os.makedirs("./outputpal/plots", exist_ok=True)  # Directory to save plotted images

for idx, d in enumerate(test_dataset_dicts[:5]):  # Visualize first 5 test images
    img = cv2.imread(d["file_name"])
    outputs = predictor(img)
    v = Visualizer(img[:, :, ::-1], metadata=test_metadata, scale=0.8)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    plt.figure(figsize=(14, 10))
    plt.imshow(out.get_image()[:, :, ::-1])
    plt.axis('off')

    plot_path = f"./output/plots/prediction_{idx}.png"
    plt.savefig(plot_path)  # Save the plot
    plt.show()

import os
import random
import cv2
import matplotlib.pyplot as plt
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.utils.visualizer import Visualizer

test_metadata = MetadataCatalog.get("custom_test")
test_dataset_dicts = DatasetCatalog.get("custom_test")

os.makedirs("./outputpal/plots", exist_ok=True)  # Directory to save plotted images

# Select 10 random entries from the test dataset
random_entries = random.sample(test_dataset_dicts, 10)

for idx, d in enumerate(random_entries):  # Visualize 10 random test images
    img = cv2.imread(d["file_name"])
    outputs = predictor(img)
    v = Visualizer(img[:, :, ::-1], metadata=test_metadata, scale=0.8)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    plt.figure(figsize=(14, 10))
    plt.imshow(out.get_image()[:, :, ::-1])
    plt.axis('off')

    plot_path = f"./outputpal/plots/prediction_{idx}.png"
    plt.savefig(plot_path)  # Save the plot
    plt.show()

# Step 3: Upload images
from google.colab import files
import os

# Upload images
uploaded = files.upload()

# Create a directory to store uploaded images
os.makedirs("uploaded_images", exist_ok=True)

# Save uploaded images to the directory
for filename in uploaded.keys():
    with open(os.path.join("uploaded_images", filename), 'wb') as f:
        f.write(uploaded[filename])

# Step 4: Load the trained model and run inference
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
import matplotlib.pyplot as plt
import cv2


# Run the model on uploaded images
for filename in uploaded.keys():
    img_path = os.path.join("uploaded_images", filename)
    img = cv2.imread(img_path)
    outputs = predictor(img)

    v = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    plt.figure(figsize=(10, 10))
    plt.imshow(out.get_image()[:, :, ::-1])
    plt.axis('off')
    plt.title(filename)
    plt.show()

"""# Troubleshooting"""

# Run the following if you see a locale error
import locale
locale.getpreferredencoding = lambda: "UTF-8"